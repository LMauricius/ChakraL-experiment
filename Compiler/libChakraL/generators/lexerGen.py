import os
from genUtil import *
from typing import Tuple

class Command:
    def __init__(self, name: str, arg: str):
        self.name = name
        self.arg = arg

class Rule:
    def __init__(self, regex: str):
        self.regex = regex
        self.commands = list[Command]()

class Lexer:
    def __init__(self):
        self.macros = dict[str, str]()
        self.regexNames = dict[str, str]()
        self.regexTokenPairs = list[Tuple[str, str]]()
        self.tokenTypes = list[str]()
        self.states = list[str]()
        self.starting = ""
        self.rulesPerState = dict[str, list[Rule]]()

        self.macros["[:newl:]"] = "\\n"
        self.macros["[:alnum:]"] = "a-zA-Z0-9"
        self.macros["[:alpha:]"] = "a-zA-Z"
        self.macros["[:ascii:]"] = "\\x00-\\x7F"
        self.macros["[:blank:]"] = " \\t"
        self.macros["[:cntrl:]"] = "\\x00-\\x1F\\x7F"
        self.macros["[:digit:]"] = "0-9"
        self.macros["[:graph:]"] = "\\x21-\\x7E"
        self.macros["[:lower:]"] = "a-z"
        self.macros["[:print:]"] = "\\x20-\\x7E"
        self.macros["[:punct:]"] = "!\"\#$%&'()*+,\\-./:;<=>?@\\[\\\\\\]^_â€˜{|}~"
        self.macros["[:space:]"] = " \\t\\r\\n\\v\\f"
        self.macros["[:upper:]"] = "A-Z"
        self.macros["[:word:]"] = "A-Za-z0-9_"
        self.macros["[:xdigit:]"] = "A-Fa-f0-9"

def replaceMacros(regex: str, macros: dict[str, str]):
    ret = ""

    i = 0
    while i < len(regex):
        if regex[i:i+2] == '[:':
            ind = regex.find(']', i+1)
            if regex[ind-1] == ':':
                end = ind+1
                ret += macros[regex[i:end]]
                i = end
            else:
                ret += regex[i]
                i += 1
        else:
            ret += regex[i]
            i += 1


    return ret



def loadLexer(filename: str):
    print("...Loading lexer ", filename)

    lines = meaningfulLines(filename)
    lexer = Lexer()
    curState = ""

    try:
        lineInd = 0
        l, lineInd = next(lines, ("",-1))
        while len(l):
            #print("...Line ", l)

            if l[0] == '[':
                l2, lineInd = next(lines, ("",-1))
                lexer.macros[l] = replaceMacros(l2, lexer.macros)
            elif l[0] == '@':
                state = l[1:]
                #print("...Entered state ", state)
                if (len(lexer.states) == 0):
                    lexer.starting = state
                if not state in lexer.states:
                    lexer.states.append(state)
                lexer.rulesPerState[state] = []
                curState = state
            elif l[0] == '?':
                # regex
                regex = replaceMacros(l[1:], lexer.macros)

                if not regex in lexer.regexNames:
                    lexer.regexNames[regex] = "ruleRegex" + str(len(lexer.regexNames))

                # create rule
                rule = Rule(regex)
                #print("...Rule with regex ", regex)

                l, lineInd = next(lines, ("",-1))
                if l != '{':
                    raise FormatError("Expected a '{', got '"+l+"'", lineInd)
                
                # commands
                l, lineInd = next(lines, ("",-1))
                while l != '}':
                    spl = l.split()
                    if len(spl) > 1:
                        rule.commands.append(Command(spl[0], spl[1]))
                    else:
                        rule.commands.append(Command(spl[0], ""))
                    
                    if spl[0] == 'out':
                        if not spl[1] in lexer.tokenTypes:
                            lexer.tokenTypes.append(spl[1])
                        lexer.regexTokenPairs.append((regex, spl[1]))

                    l, lineInd = next(lines, ("",-1))
                    if len(l) == 0:
                        raise FormatError("Expected a '}', got '"+l+"'", lineInd)


                # add rule
                lexer.rulesPerState[curState].append(rule)

            l, lineInd = next(lines, ("",-1))
    except FormatError as e:
        print("At line " + str(e.line) + ": " + e.message)
    except StopIteration:
        pass
    
    return lexer




def writeLexerH(lexer: Lexer, filename: str):
    print("...Writing lexer ", filename)
    f = open(filename, "wt")

    def TB():
        f.write('    ')
    def LN(s: str):
        f.write(s)
        f.write('\n')

    LN("// This file is autogenerated. Do not edit!")
    LN("")
    LN('#include <list>')
    LN('#include <string>')
    LN('#include <vector>')
    LN('#include <array>')
    LN("")
    LN("namespace ChakraL {")
    LN("")
    TB();LN("enum class TokenType {")
    for t in lexer.tokenTypes:
        TB();TB();LN(t+',')
    TB();LN("};")
    LN("")
    TB();LN("extern const std::array<std::string, "+str(len(lexer.tokenTypes))+"> TokenNames;")
    LN("")
    TB();LN("enum class LexerState {")
    for t in lexer.states:
        TB();TB();LN(t+',')
    TB();LN("};")
    LN("")
    TB();LN("class Token {")
    TB();LN('public:')
    TB();TB();LN('TokenType type;')
    TB();TB();LN('std::wstring str;')
    TB();TB();LN('int line;')
    TB();TB();LN('int character;')
    TB();LN("};")
    LN("")
    TB();LN("class LexerError {")
    TB();LN('public:')
    TB();TB();LN('inline LexerError(int line, int character, std::wstring msg): line(line), character(character), msg(msg) {{}}')
    TB();TB();LN('std::wstring msg;')
    TB();TB();LN('int line;')
    TB();TB();LN('int character;')
    TB();LN("};")
    LN("")
    TB();LN("std::list<Token> tokenize(const std::wstring &input, std::list<LexerError>& outErrors);")
    LN("")
    LN("}")

    f.close()


def writeLexerCPP(lexer: Lexer, filename: str, headerfile: str):
    print("...Writing lexer ", filename)
    f = open(filename, "wt")

    def TB():
        f.write('    ')
    def LN(s: str):
        f.write(s)
        f.write('\n')

    LN("// This file is autogenerated. Do not edit!")
    LN("")
    LN('#include "ctre-unicode.hpp"')
    LN('#include "' + os.path.basename(headerfile) + '"')
    #LN('#include "ctre.hpp"')
    LN('#include <vector>')
    LN('#include <string_view>')
    #LN('#include <regex>') # obsolete
    LN('#include <sstream>')
    #LN('#include "ctre-unicode.hpp"')
    LN('#include <iostream>')
    LN("")
    LN("namespace")
    LN("{")
    #for reg in lexer.regexNames:
    #    TB();LN("const std::wregex " + lexer.regexNames[reg] + "(L\"" + escapeRegexToC(reg) + "\", std::wregex::extended);")
    LN("}")
    LN("namespace ChakraL")
    LN("{")
    TB();LN("const std::array<std::string, "+str(len(lexer.tokenTypes))+"> TokenNames {")
    for t in lexer.tokenTypes:
        TB();TB();LN('"'+t+'",')
    TB();LN("};")
    LN("")
    TB();LN("std::list<Token> tokenize(const std::wstring &input, std::list<LexerError>& outErrors)")
    TB();LN("{")
    TB();TB();LN("int line = 1, character = 1;")
    TB();TB();LN("int start = 0, maxEnd = -1;")
    TB();TB();LN("std::vector<LexerState> states = {{LexerState::" + lexer.starting + "}};")
    TB();TB();LN("std::list<Token> ret;")
    TB();TB();LN("")
    TB();TB();LN("while (start < input.size())")
    TB();TB();LN("{")
    TB();TB();TB();LN("int choosenRuleInd = -1;")
    TB();TB();TB();LN("bool movePtr = true;")
    '''
    TB();TB();TB();LN("std::wsmatch match;")
    '''
    TB();TB();TB();LN("std::wstring_view matchStr;")
    TB();TB();TB();LN("")
    TB();TB();TB();LN("switch (states.back())")
    TB();TB();TB();LN("{")
    for s in lexer.states:
        TB();TB();TB();LN("case LexerState::"+s+":")

        TB();TB();TB();TB();LN("")
        TB();TB();TB();TB();LN("// *** choose rule ***")
        i = 0

        # Faster method

        # Slower method
        
        for r in lexer.rulesPerState[s]:
            i += 1
            #TB();TB();TB();TB();LN("{")
            #TB();TB();TB();TB();TB();LN("std::wstring_view part = std::wstring_view(input.begin()+start, input.end());")
            #TB();TB();TB();TB();TB();LN("auto res = ctre::starts_with<L\"(" + escapeRegexToC(r.regex) + ")\">(part);")
            #TB();TB();TB();TB();TB();LN("std::wstring_view str = res.to_view();")
            #std::wstring_view(input.begin()+start, input.end())
            # && res.to_view().length() > (maxEnd-start)
            TB();TB();TB();TB();LN("if (auto res = ctre::starts_with<L\"(" + escapeRegexToC(r.regex) + ")\">(std::wstring_view(input.begin()+start, input.end())); res && (int)res.size() > (maxEnd-start)) {")
            #TB();TB();TB();TB();LN("int sz = res.size(); int maxL = (maxEnd-start);")
            #TB();TB();TB();TB();LN("if (res) {")
            #TB();TB();TB();TB();LN("if ((int)res.size() > (maxEnd-start)) {")
            #TB();TB();TB();TB();LN("if (res) {")
            TB();TB();TB();TB();TB();LN("choosenRuleInd = " + str(i) + ";")
            TB();TB();TB();TB();TB();LN("maxEnd = start + res.size();")
            #TB();TB();TB();TB();TB();LN("std::wcout << L\"matched: " + escapeRegexToC(r.regex) + "\" << std::endl;")
            TB();TB();TB();TB();LN("}")
            #TB();TB();TB();TB();LN("}")
            #TB();TB();TB();TB();LN("}")
            '''
            TB();TB();TB();TB();LN("if (std::regex_search(input.begin() + start, input.end(), match, " + lexer.regexNames[r.regex] + ", std::regex_constants::match_continuous) && match[0].length() > (maxEnd-start)) {")
            TB();TB();TB();TB();TB();LN("choosenRuleInd = " + str(i) + ";")
            TB();TB();TB();TB();TB();LN("maxEnd = start + match[0].length();")
            TB();TB();TB();TB();LN("}")
            '''
             
        
        TB();TB();TB();TB();LN("")
        TB();TB();TB();TB();LN("// *** execute rule commands ***")
        TB();TB();TB();TB();LN("switch (choosenRuleInd)")
        TB();TB();TB();TB();LN("{")
        i = 0
        for r in lexer.rulesPerState[s]:
            i += 1
            TB();TB();TB();TB();LN("case " + str(i) + ":// " + escapeRegexToCComment(r.regex))
            for c in r.commands:
                if c.name == "out":
                    TB();TB();TB();TB();TB();LN("ret.push_back(Token{TokenType::" + c.arg + ", input.substr(start, maxEnd-start), line, character});")
                    #TB();TB();TB();TB();TB();LN("std::cout << \"token: " + c.arg + "\" << std::endl;")
                elif c.name == "push":
                    TB();TB();TB();TB();TB();LN("states.push_back(LexerState::" + c.arg + ");")
                elif c.name == "pop":
                    TB();TB();TB();TB();TB();LN("states.pop_back();")
                elif c.name == "swap":
                    TB();TB();TB();TB();TB();LN("states.pop_back();")
                elif c.name == "throw":
                    TB();TB();TB();TB();TB();LN("movePtr = false;")
            TB();TB();TB();TB();TB();LN("break;")
        TB();TB();TB();TB();LN("default:")
        TB();TB();TB();TB();LN("{")
        TB();TB();TB();TB();TB();LN("maxEnd = start+1;")
        TB();TB();TB();TB();TB();LN("movePtr = true;")
        TB();TB();TB();TB();TB();LN("std::wstringstream ss;")
        TB();TB();TB();TB();TB();LN("ss << \"Unexpected character '\" << input[start] << \"'\";")
        TB();TB();TB();TB();TB();LN("outErrors.emplace_back(line, character, ss.str());")
        TB();TB();TB();TB();LN("}")
        TB();TB();TB();TB();LN("}")
        TB();TB();TB();TB();LN("break;")
    TB();TB();TB();LN("}")
    
        
    TB();TB();TB();LN("")
    TB();TB();TB();LN("// *** count lines/positions ***")
    TB();TB();TB();LN("for (int i=start; i<maxEnd; i++)")
    TB();TB();TB();LN("{")
    TB();TB();TB();TB();LN("if (input[i] == '\\n')")
    TB();TB();TB();TB();LN("{")
    TB();TB();TB();TB();TB();LN("line++;")
    TB();TB();TB();TB();TB();LN("character = 0;")
    TB();TB();TB();TB();LN("}")
    TB();TB();TB();TB();LN("character++;")
    TB();TB();TB();LN("}")
    TB();TB();TB();LN("")
    TB();TB();TB();LN("// *** move pointer ***")
    TB();TB();TB();LN("if (movePtr) {")
    TB();TB();TB();TB();LN("start = maxEnd;")
    TB();TB();TB();TB();LN("maxEnd = -1;")
    TB();TB();TB();LN("}")
    TB();TB();LN("}")
    TB();TB();LN("")
    TB();TB();LN("return ret;")
    TB();LN("}")
    LN("")
    LN("}")

    f.close()